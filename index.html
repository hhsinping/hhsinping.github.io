<!DOCTYPE HTML>
<html lang="en">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Hsin-Ping Huang</title>
      <meta name="author" content="Hsin-Ping Huang">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="stylesheet"
         href="https://fonts.googleapis.com/css2?family=Google+Sans:wght@700&family=Noto+Sans:wght@400;500;600;700&display=swap">
      <link rel="stylesheet" type="text/css" href="stylesheet.css">
      <link rel="icon" type="image/png" href="images/seal_icon.png">
   </head>
   <body>
      <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
         <tbody>
            <tr style="padding:0px">
               <td style="padding:0px">
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr style="padding:0px">
                           <td style="padding:2.5%;width:30%;max-width:30%">
                              <a href="images/photo.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo.png" class="hoverZoomLink"></a>
                           </td>
                           <td style="padding:2.5%;width:70%;vertical-align:middle">
                              <p style="text-align:center">
                                 <name>Hsin-Ping Huang</name>
                              </p>
                              <p>
                                 I am a fifth-year Ph.D. candidate in the <a class="color" href="http://vllab.ucmerced.edu/">Vision and Learning Lab</a> at University of California, Merced, under the supervision of Prof. <a class="color" href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>. Previously, I received my M.S. in Computer Science from The University of Texas at Austin and my B.S. in Electrical Engineering from National Taiwan University.
                              </p>
                              <p>
                                 My research interests lie in computer vision and machine learning, with a focus on image, video and 3D generation and manipulation. I am fortunate to intern at Google with <a class="color" href="https://deqings.github.io/">Deqing Sun</a>, <a class="color" href="https://sammy-su.github.io/">Yu-Chuan Su</a>,  <a class="color" href="https://www.hexianghu.com/">Hexiang Hu</a>, <a class="color" href="http://www.lujiang.info/">Lu Jiang</a>, <a class="color" href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ&hl=en">Charles Herrmann</a>, <a class="color" href="https://yaojieliu.github.io/">Yaojie Liu</a>, and <a class="color" href="https://cindyxinyiwang.github.io/">Xinyi Wang</a>, at Adobe Research with <a class="color" href="https://people.cs.umass.edu/~zhanxu/">Zhan Xu</a> and <a class="color" href="https://yzhou359.github.io/">Yang Zhou</a>, and to receive advice from <a class="color" href="https://hytseng0509.github.io/">Hung-Yu Tseng</a> and <a class="color" href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>.
                              </p>
                              <p>
                                 I am honored to be a finalist for the <a class="color" href="https://research.facebook.com/blog/2022/2/announcing-the-recipients-of-the-2022-meta-phd-research-fellowship/">Meta PhD Research Fellowship</a>.
                              </p>
                              <p class="color">
                                 I am seeking full-time positions in 2025.
                              </p>
                              <p style="text-align:center">
                                 <a class="color"  href="mailto:hhuang79@ucmerced.edu">Email</a> &nbsp/&nbsp
                                 <a class="color"  href="data/cv.pdf">CV</a>&nbsp/&nbsp
                                 <a class="color"  href="https://scholar.google.com/citations?user=nGCt2tkAAAAJ&hl=en">Google Scholar</a>&nbsp/&nbsp
                                 <a class="color"  href="https://www.linkedin.com/in/hsin-ping-huang-4aa27016b/">LinkedIn</a>&nbsp/&nbsp
                                 <a class="color"  href="https://github.com/hhsinping">GitHub</a>
                              </p>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:0px;width:100%;vertical-align:middle">
                              <table class="container"
                                 style="max-width:900ox;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                                 <th width=90px valign="top" align="center">
                                    <img src="images/ntu.png" alt="sym" width=80px></a>
                                    <p style="line-height:1.3; font-size:10pt; font-weight:500">National Taiwan University<br>B.S. in EE<br> 2013 - 2017</p>
                                 </th>
                                 <th width=90px valign="top" align="center">
                                    <img src="images/ut.png" alt="sym" width=80px></a>
                                    <p style="line-height:1.3; font-size:10pt; font-weight:500">University of Texas at Austin<br>M.S. in CS<br> 2017 - 2020</p>
                                 </th>
                                 <th width=90px valign="top" align="center">
                                    <img src="images/ucmerced.png" alt="sym" width=80px></a>
                                    <p style="line-height:1.3; font-size:10pt; font-weight:500">University of California, Merced<br>Ph.D. in EECS<br>2020 - Present</p>
                                 </th>
                                 <th width=90px valign="top" align="center">
                                    <img src="images/amazon.png" alt="sym" width=80px></a>
                                    <p style="line-height:1.3; font-size:10pt; font-weight:500">Amazon<br>Applied Scientist Intern<br>May 2020 - Aug. 2020</p>
                                 </th>
                                 <th width=90px valign="top" align="center">
                                    <img src="images/google.png" alt="sym" width=80px></a>
                                    <p style="line-height:1.3; font-size:10pt; font-weight:500">Google Research / DeepMind<br>Student Researcher<br>May 2021 - May 2024</p>
                                 </th>
                                 <th width=90px valign="top" align="center">
                                    <img src="images/adobe.jpg" alt="sym" width=80px></a>
                                    <p style="line-height:1.3; font-size:10pt; font-weight:500">Adobe Research<br>Research Intern<br>Jun. 2024 - Nov. 2024</p>
                                 </th>
                              </table>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:100%;vertical-align:middle">
                              <br>
                              <heading>Publications</heading>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <video width="280" height=100% muted autoplay loop>
                                 <source src="images/move.mp4" type="video/mp4">
                              </video>
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Move-in-2D: 2D-Conditioned Human Motion Generation</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, 
                              <a href="https://yzhou359.github.io/">Yang Zhou</a>,
                              <a href="http://juiwang.com/">Jui-Hsien Wang</a>,
                              <a href="https://difanliu.github.io/">Difan Liu</a>,
                              <a href="https://pages.cs.wisc.edu/~fliu/">Feng Liu</a>,
                              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>,
							  <a href="https://scholar.google.com/citations?user=pF2vMhgAAAAJ">Zhan Xu</a>
                              <br>
                              <em>arXiv 2024</em>
                              <br>
							  <span>Move-in-2D generates human motion sequences conditioned on a scene image and text prompt, using a diffusion model trained on a large-scale dataset of annotated human motions.</span>
							  <br>
                              <a class="color" href="https://hhsinping.github.io/Move-in-2D/">Project Page</a> &nbsp/&nbsp
                              <a class="color" href="https://arxiv.org/abs/2412.13185">Paper</a> 
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <img src="images/kitten.png" width="280">
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, 
                              <a href="https://cindyxinyiwang.github.io/">Xinyi Wang</a>, 
                              <a href="https://yonatanbitton.github.io/">Yonatan Bitton</a>, 
                              <a href="https://scholar.google.com/citations?user=OLCuVVoAAAAJ&hl=en">Hagai Taitelbaum</a>, 
                              <a href="https://scholar.google.com/citations?user=p1SDN0oAAAAJ&hl=en">Gaurav Singh Tomar</a>, 
                              <a href="https://scholar.google.com/citations?user=GiCqMFkAAAAJ&hl=en">Ming-Wei Chang</a>, 
                              <a href="https://scholar.google.com/citations?user=vO0VSSYAAAAJ&hl=en">Xuhui Jia</a>, 
                              <a href="https://ckkelvinchan.github.io/">Kelvin C.K. Chan</a>, 
                              <a href="https://www.hexianghu.com/">Hexiang Hu</a>, 
                              <a href="https://sammy-su.github.io/">Yu-Chuan Su</a>, 
                              <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
                              <br>
                              <em>arXiv 2024</em>
                              <br>
							  <span>KITTEN is a benchmark for evaluating text-to-image models' ability to generate real-world visual entities, highlighting that even advanced models struggle with entity fidelity.</span>
							  <br>
                              <a class="color" href="https://kitten-project.github.io/">Project Page</a> &nbsp/&nbsp
                              <a class="color" href="https://arxiv.org/abs/2410.11824">Paper</a> 
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <video width="280" height=100% muted autoplay loop>
                                 <source src="images/factor.mp4" type="video/mp4">
                              </video>
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Fine-grained Controllable Video Generation via Object Appearance and Context</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, <a href="https://sammy-su.github.io/">Yu-Chuan Su</a>, <a href="https://deqings.github.io/">Deqing Sun</a>, <a href="http://www.lujiang.info/">Lu Jiang</a>, <a href="https://scholar.google.com/citations?user=vO0VSSYAAAAJ&hl=en">Xuhui Jia</a>, <a href="https://scholar.google.com/citations?user=_CuXgYIAAAAJ&hl=en">Yukun Zhu</a>, <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
                              <br>
                              <em>WACV 2025</em>
                              <br>
							  <span>FACTOR is a video generation model that allows detailed control over objects' appearances, context, and location by optimizing the inserted attention layer with large-scale annotations.</span>
							  <br>
                              <a class="color" href="https://hhsinping.github.io/factor/">Project Page</a> &nbsp/&nbsp
                              <a class="color" href="data/factor.pdf">Paper</a> &nbsp/&nbsp
                              <a class="color" href="https://x.com/_akhaliq/status/1732223789499322573">Media (AK)</a> 
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <img src="images/long.png" width="280">
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Generating Long-take Videos via Effective Keyframes and Guidance</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, <a href="https://sammy-su.github.io/">Yu-Chuan Su</a>, <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
                              <br>
                              <em>WACV 2025</em>
                              <br>
							  <span>We propose a framework for generating long-take videos with multiple coherent events by decoupling video generation into keyframe generation and frame interpolation.</span>
                              <br>
                              <a class="color" href="data/long.pdf">Paper</a>  &nbsp/&nbsp
                              <a class="color" href="https://x.com/_akhaliq/status/1648185407572508672">Media (AK)</a>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <img src="images/saf.png" width="280">
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Self-supervised AutoFlow</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, <a href="https://scholar.google.com/citations?user=LQvi5XAAAAAJ&hl=en">Charles Herrmann</a>, <a href="https://hurjunhwa.github.io/">Junhwa Hur</a>, <a href="https://erikalu.com/">Erika Lu</a>, <a href="https://kylesargent.github.io/">Kyle Sargent</a>, <a href="https://scholar.google.com/citations?user=NLOh3SUAAAAJ&hl=en">Austin Stone</a>, <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>, <a href="https://deqings.github.io/">Deqing Sun</a>
							  <br>
                              <em>CVPR 2023</em>
                              <br>
							  <span>Self-supervised AutoFlow is a framework for learning optical flow training datasets that replaces the need for ground truth labels by using a self-supervised loss as the search metric.</span>
                              <br>
                              <a class="color" href="https://autoflow-google.github.io/">Project Page</a> &nbsp/&nbsp
                              <a class="color" href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Self-Supervised_AutoFlow_CVPR_2023_paper.html">Paper</a>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <img src="images/fas.png" width="280">
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Adaptive Transformers for Robust Few-shot Cross-domain Face Anti-spoofing</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, <a href="https://deqings.github.io/">Deqing Sun</a>, <a href="https://yaojieliu.github.io/">Yaojie Liu</a>, <a href="https://l2ior.github.io/">Wen-Sheng Chu</a>, <a href="https://prinsphield.github.io/">Taihong Xiao</a>, <a href="https://scholar.google.com/citations?user=NUdlxaIAAAAJ&hl=en">Jinwei Yuan</a>, <a href="https://scholar.google.com/citations?user=fWd88tEAAAAJ&hl=en">Hartwig Adam</a>, <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
                              <br>
                              <em>ECCV 2022</em>
							  <br>
							  <span>We present adaptive vision transformers for face anti-spoofing, introducing ensemble adapters and feature-wise transformation layers for domain adaptation with few samples.</span>
                              <br>
                              <a class="color" href="https://github.com/hhsinping/few_shot_fas">Project Page</a> &nbsp/&nbsp
                              <a class="color" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5418_ECCV_2022_paper.php">Paper</a>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <video width="280" height=100% muted autoplay loop>
                                 <source src="images/tank.mp4" type="video/mp4">
                              </video>
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Learning to Stylize Novel Views</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, <a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a>, <a href="https://sophont01.github.io/">Saurabh Saini</a>, <a href="https://scholar.google.com/citations?user=hdQhiFgAAAAJ&hl=en">Maneesh Singh</a>, <a href="https://faculty.ucmerced.edu/mhyang/">Ming-Hsuan Yang</a>
                              <br>
                              <em>ICCV 2021</em>
							  <br>
							  <span>We tackle 3D scene stylization, generating stylized images from novel views by constructing a point cloud, aggregating style statistics, and modulating features with a linear transformation.</span>
                              <br>
                              <a class="color" href="https://hhsinping.github.io/3d_scene_stylization/">Project Page</a> &nbsp/&nbsp
                              <a class="color" href="https://openaccess.thecvf.com/content/ICCV2021/html/Huang_Learning_To_Stylize_Novel_Views_ICCV_2021_paper.html">Paper</a>  &nbsp/&nbsp
                              <a class="color" href="https://x.com/_akhaliq/status/1399210896103256064">Media (AK)</a>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <img src="images/aec.png" width="280">
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Unsupervised and Semi-Supervised Few-Shot Acoustic Event Classification</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, <a href="https://scholar.google.com/citations?user=mRVm3tkAAAAJ&hl=en">Krishna C. Puvvada</a>, <a href="https://scholar.google.com/citations?user=YvyOsEUAAAAJ&hl=en">Ming Sun</a>,  <a href="https://scholar.google.com/citations?user=Ucw6EJAAAAAJ&hl=en">Chao Wang</a>
                              <br>
                              <em>ICASSP 2021</em>
                              <br>
							  <span>We study semi-supervised few-shot acoustic event classification, learning audio representations from a large amount of unlabeled data and using these representations for classification.</span>
							  <br>
                              <a class="color" href="https://www.amazon.science/publications/unsupervised-and-semi-supervised-few-shot-acoustic-event-classification">Paper</a>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <video width="280" height=100% muted autoplay loop>
                                 <source src="images/svs.mp4" type="video/mp4">
                              </video>
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Semantic View Synthesis</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, <a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a>, <a href="http://hsinyinglee.com/">Hsin-Ying Lee</a>, <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>
                              <br>
                              <em>ECCV 2020</em>
                              <br>
							  <span>We address semantic view synthesis: generating free-viewpoint renderings of a synthesized scene from a semantic label map by synthesizing a multiple-plane image (MPI) representation.<span>
							  <br>
                              <a class="color" href="https://hhsinping.github.io/svs/">Project Page</a> &nbsp/&nbsp
                              <a class="color" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1653_ECCV_2020_paper.php">Paper</a>  &nbsp/&nbsp
                              <a class="color" href="https://x.com/_akhaliq/status/1298081936800448512">Media (AK)</a>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="width:30%;vertical-align:middle">
                              <div class="one">
                              <img src="images/dis.png" width="280">
                              </div>
                           </td>
                           <td width="70%" valign="middle" height=210px>
                              <papertitle>Unsupervised Adversarial Domain Adaptation for Implicit Discourse Relation Classification</papertitle>
                              <br>
                              <strong>Hsin-Ping Huang</strong>, <a href="https://jessyli.com/">Junyi Jessy Li</a>
                              <br>
                              <em>CoNLL 2019</em>
							  <br>
							  <span>We present an unsupervised adversarial domain adaptive network with a reconstruction component that leverages explicit discourse relations to classify implicit discourse relations.</span>
                              <br>
							  <a class="color" href="https://aclanthology.org/K19-1064/">Paper</a>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:0px;width:100%;vertical-align:middle">
                              <br>
                              <heading>Honors and Awards</heading>
							  <br><br>
                              <span>UC Merced Bobcat Fellowship, 2024 </span>
							  <br>
                              <span><a class="color"href="https://research.facebook.com/blog/2022/2/announcing-the-recipients-of-the-2022-meta-phd-research-fellowship/">Meta PhD Research Fellowship</a> Finalist - AR/VR Human Understanding, 2022</span>
							  <br><br>                              
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <hr>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:0px;width:100%;vertical-align:middle">
                              <br>
                              <heading>Professional Activities</heading>
							  <br><br>
                              <span>Journal Reviewer: TPAMI, IJCV, CVIU, Computer Graphics Forum</span>
							  <br>
							  <span>Conference Reviewer (Computer Vision): ECCV'24, ICCV'23, CVPR'23, ECCV'22, CVPR'22, ICCV'21</span>
							  <br>
							  <span>Conference Reviewer (Artificial Intelligence): IJCAI'24, AAAI'24, IJCAI'23, AAAI'23</span>
                              <br><br>
                           </td>
                        </tr>
                     </tbody>
                  </table>
                  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                     <tbody>
                        <tr>
                           <td style="padding:0px">
                              <br>
                              <p style="text-align:right;font-size:small;">
                                 This page borrows designs from <a class='small' href="https://jonbarron.info/">Jon Barron</a>'s website.
                              </p>
                           </td>
                        </tr>
                     </tbody>
                  </table>
               </td>
            </tr>
      </table>
   </body>
</html>
